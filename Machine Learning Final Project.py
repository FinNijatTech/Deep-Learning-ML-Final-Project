# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14I37T9tfEf5G99eP16PxgxrF6Vg6ac-n
"""

import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from collections import defaultdict
import seaborn as sns
import pandas_datareader.data as web
from sklearn.ensemble import GradientBoostingClassifier
from datetime import datetime
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import warnings
warnings.filterwarnings('ignore')

SP500 = yf.download("^GSPC", start="1990-01-01", end="2022-01-01")

SP500 = SP500.reset_index()

"""**Exploratory Data Analysis**"""

SP500.head()

SP500.info()

# Change Dtype of Date column
SP500["Date"] = pd.to_datetime(SP500["Date"])

plt.figure(figsize=(10, 8))
plt.plot(SP500["Date"], SP500["Adj Close"])
plt.title('S&P 500')
plt.ylabel('Adj Close')

# Now lets plot the total volume of stock being traded each day
plt.figure(figsize=(10, 8))
plt.plot(SP500["Date"], SP500["Volume"])
plt.title('SP500')
plt.ylabel('Volume')

"""What was the daily return of the index on average ?"""

SP500["Daily Return"] = SP500["Adj Close"].pct_change()

plt.figure(figsize=(10, 8))
plt.plot(SP500["Date"], SP500["Daily Return"])
plt.title("S&P 500")
plt.ylabel('Daily Return')

plt.figure(figsize=(10, 8))
sns.distplot(SP500["Daily Return"].dropna(), color = "purple")
plt.title("S&P 500")

print("Kurtosis Value")
print(f"S&P500: {SP500['Daily Return'].kurtosis()}")

SP500_returns = pd.DataFrame()
SP500_returns["S&P500"] = SP500["Adj Close"]
SP_500_returns = SP500_returns.pct_change()
SP_500_returns.head()

SP500.set_index('Date', inplace=True)

SP500

# Calculate 50-day and 200-day moving averages
SP500["SMA_50"] = SP500["Close"].rolling(window=50).mean()
SP500["SMA_200"] = SP500["Close"].rolling(window=200).mean()

# Calculate the RSI (Relative Strength Index) for a 14-day period
rsi_period = 14
delta = SP500["Adj Close"].diff(1)
gain = delta.where(delta > 0, 0)
loss = -delta.where(delta < 0, 0)
average_gain = gain.rolling(window=rsi_period).mean()
average_loss = loss.rolling(window=rsi_period).mean()
relative_strength = average_gain / average_loss
rsi = 100 - (100 / (1 + relative_strength))
SP500["RSI"] = rsi

# Display the data
print(SP500.tail())

# Plot the closing price, moving averages, and RSI
plt.figure(figsize=(12, 6))
plt.plot(SP500.index, SP500["Adj Close"], label="S&P 500 Adj Close", color='blue')
plt.plot(SP500.index, SP500["SMA_50"], label="50-day SMA", color='orange')
plt.plot(SP500.index, SP500["SMA_200"], label="200-day SMA", color='red')
plt.legend()
plt.title("S&P 500 Historical Data with Moving Averages")
plt.xlabel("Date")
plt.ylabel("Price")
plt.grid(True)

plt.figure(figsize=(12, 3))
plt.plot(SP500.index, SP500["RSI"], label="RSI", color='green')
plt.axhline(70, color='red', linestyle='--', alpha=0.5)
plt.axhline(30, color='green', linestyle='--', alpha=0.5)
plt.fill_between(SP500.index, 30, 70, alpha=0.2, color='yellow')
plt.legend()
plt.title("Relative Strength Index (RSI)")
plt.xlabel("Date")
plt.ylabel("RSI Value")
plt.grid(True)

plt.show()

df = SP500[['Adj Close', 'Daily Return', 'SMA_50', 'SMA_200', 'RSI']]

df.dropna(inplace=True)

df

df

df = df.astype(int)

# x = df[['Daily Return', 'SMA_50', 'SMA_200', 'RSI']]
# y = df['Adj Close']

# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

df = df.reset_index()

df['Date'] = pd.to_datetime(df['Date'])

# Split data into training (1990-01-01 to 2019-12-31) and test (2020-01-01 to 2022-01-01) sets
train_data = df[(df['Date'] >= '1990-01-01') & (df['Date'] <= '2019-12-31')]
test_data = df[(df['Date'] >= '2020-01-01') & (df['Date'] <= '2022-01-01')]

x_train = train_data[['RSI', 'SMA_50', 'SMA_200']]
y_train = train_data['Adj Close']

x_test = test_data[['RSI', 'SMA_50', 'SMA_200']]
y_test =  test_data['Adj Close']

"""**KNN**"""

%%time

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()

param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'p': [1, 2]
}

knn_grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')

knn_grid.fit(x_train, y_train)

best_knn_params = knn_grid.best_params_
best_knn_score = knn_grid.best_score_

print("Best hyperparameters for KNN:", best_knn_params)
print("Best accuracy score for KNN:", best_knn_score)

knn = KNeighborsClassifier(n_neighbors= 5, p= 1, weights='uniform')
knn.fit(x_train, y_train)
prediction_knn = knn.predict(x_test)

# Create DataFrames for predictions and actual values
prediction_df = pd.DataFrame({'Date': test_data['Date'], 'Predicted_Price': prediction_knn})
y_test_df = pd.DataFrame({'Date': test_data['Date'], 'Actual_Price': y_test})

# Merge the two DataFrames on 'Date'
merged_df = pd.merge(prediction_df, y_test_df, on='Date')

# Plot the two time series data
plt.figure(figsize=(12, 6))
plt.plot(merged_df['Date'], merged_df['Predicted_Price'], label='Predicted Price', color='blue')
plt.plot(merged_df['Date'], merged_df['Actual_Price'], label='Actual Price', color='green')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.title('KNN Model Forecast vs. Actual Stock Prices')
plt.legend()
plt.grid(True)
plt.show()

"""**ADA Boosting**"""

%%time

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

# AdaBoost
adaboost = AdaBoostClassifier()
param_grid_adaboost = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.01, 0.1, 0.2]
}

adaboost_grid = GridSearchCV(adaboost, param_grid_adaboost, cv=5, scoring='f1')
adaboost_grid.fit(x_train, y_train)
best_adaboost_params = adaboost_grid.best_params_
best_adaboost_score = adaboost_grid.best_score_

print("Best hyperparameters for AdaBoost:", best_adaboost_params)
print("Best f1 score for AdaBoost:", best_adaboost_score)

ada = AdaBoostClassifier(n_estimators=150, learning_rate=0.2)
ada.fit(x_train, y_train)
prediction_ada = ada.predict(x_test)

# Create DataFrames for predictions and actual values
prediction_df = pd.DataFrame({'Date': test_data['Date'], 'Predicted_Price': prediction_ada})
y_test_df = pd.DataFrame({'Date': test_data['Date'], 'Actual_Price': y_test})

# Merge the two DataFrames on 'Date'
merged_df = pd.merge(prediction_df, y_test_df, on='Date')

# Plot the two time series data
plt.figure(figsize=(12, 6))
plt.plot(merged_df['Date'], merged_df['Predicted_Price'], label='Predicted Price', color='blue')
plt.plot(merged_df['Date'], merged_df['Actual_Price'], label='Actual Price', color='green')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.title('KNN Model Forecast vs. Actual Stock Prices')
plt.legend()
plt.grid(True)
plt.show()





"""**LSTM**"""

SP500 = SP500.reset_index()

def Dataset(Data, Date):

  Train_Data = Data['Adj Close'][Data['Date'] < Date].to_numpy()
  Data_Train = []
  Data_Train_X = []
  Data_Train_Y = []
  for i in range(0, len(Train_Data), 5):
    try:
      Data_Train.append(Train_Data[i : i + 5])
    except:
      pass

  if len(Data_Train[-1]) < 5:
    Data_Train.pop(-1)

  Data_Train_X = Data_Train[0 : -1]
  Data_Train_X = np.array(Data_Train_X)
  Data_Train_X = Data_Train_X.reshape((-1, 5, 1))
  Data_Train_Y = Data_Train[1 : len(Data_Train)]
  Data_Train_Y = np.array(Data_Train_Y)
  Data_Train_Y = Data_Train_Y.reshape((-1, 5, 1))


  Test_Data = Data['Adj Close'][Data['Date'] >= Date].to_numpy()
  Data_Test = []
  Data_Test_X = []
  Data_Test_Y = []
  for i in range(0, len(Test_Data), 5):
    try:
      Data_Test.append(Test_Data[i : i + 5])
    except:
      pass

  if len(Data_Test[-1]) < 5:
    Data_Test.pop(-1)

  Data_Test_X = Data_Test[0 : -1]
  Data_Test_X = np.array(Data_Test_X)
  Data_Test_X = Data_Test_X.reshape((-1, 5, 1))
  Data_Test_Y = Data_Test[1 : len(Data_Test)]
  Data_Test_Y = np.array(Data_Test_Y)
  Data_Test_Y = Data_Test_Y.reshape((-1, 5, 1))

  return Data_Train_X, Data_Train_Y, Data_Test_X, Data_Test_Y

def Model():
  model = tf.keras.models.Sequential([
                                      tf.keras.layers.LSTM(200, input_shape = (5, 1), activation = tf.nn.leaky_relu, return_sequences = True),
                                      tf.keras.layers.LSTM(200, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(200, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(100, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(50, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(5, activation = tf.nn.leaky_relu)
                                      ])
  return model

model = Model()

tf.keras.utils.plot_model(model, show_shapes=True)

model.summary()

def scheduler(epoch):

  if epoch <= 150:
    lrate = (10 ** -5) * (epoch / 150)
  elif epoch <= 400:
    initial_lrate = (10 ** -5)
    k = 0.01
    lrate = initial_lrate * math.exp(-k * (epoch - 150))
  else:
    lrate = (10 ** -6)

  return lrate

epochs = [i for i in range(1, 1001, 1)]
lrate = [scheduler(i) for i in range(1, 1001, 1)]
plt.plot(epochs, lrate)

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

SP500_Date = '2020-10-01'
SP500_Train_X, SP500_Train_Y, SP500_Test_X, SP500_Test_Y = Dataset(SP500)

SP500_Model = Model()
SP500_Model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'mse', metrics = tf.keras.metrics.RootMeanSquaredError())
SP500_hist = SP500_Model.fit(SP500_Train_X, SP500_Train_Y, epochs = 260, validation_data = (SP500_Test_X, SP500_Test_Y), callbacks=[callback])

history_dict = df.history

loss = history_dict["loss"]
root_mean_squared_error = history_dict["root_mean_squared_error"]
val_loss = history_dict["val_loss"]
val_root_mean_squared_error = history_dict["val_root_mean_squared_error"]

epochs = range(1, len(loss) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2)

fig.set_figheight(5)
fig.set_figwidth(15)

ax1.plot(epochs, loss, label = 'Training Loss')
ax1.plot(epochs, val_loss, label = 'Validation Loss')
ax1.set(xlabel = "Epochs", ylabel = "Loss")
ax1.legend()

ax2.plot(epochs, root_mean_squared_error, label = "Training Root Mean Squared Error")
ax2.plot(epochs, val_root_mean_squared_error, label = "Validation Root Mean Squared Error")
ax2.set(xlabel = "Epochs", ylabel = "Loss")
ax2.legend()

plt.show()

SP500_prediction = SP500_Model.predict(SP500_Test_X)

plt.figure(figsize=(20, 5))
plt.plot(SP500['Date'][SP500['Date'] >= '2020-10-09'], SP500['Adj Close'][SP500['Date'] >= '2020-10-09'], label='Testing')
plt.plot(SP500['Date'][SP500['Date'] >= '2020-10-12'][:-1], SP500_prediction.reshape(-1)[:-2], label='Predictions')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.legend(loc='best')
plt.show()

# Assuming SP500_Test_Y and SP500_prediction are NumPy arrays
rmse = math.sqrt(mean_squared_error(SP500_Test_Y.reshape(-1, 5), SP500_prediction))
mape = np.mean(np.abs(SP500_prediction - SP500_Test_Y.reshape(-1, 5)) / np.abs(SP500_Test_Y.reshape(-1, 5)))
r_squared = r2_score(SP500_Test_Y.reshape(-1, 5), SP500_prediction)

# Number of observations
n = len(SP500_Test_Y.reshape(-1, 5))

# Number of features (assuming 5 features based on the reshape)
k = 5

# Calculate adjusted R-squared
adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)

print(f'RMSE: {rmse}')
print(f'MAPE: {mape}')
print(f'Adjusted R-squared: {adjusted_r_squared}')





def Dataset(Data, Date):

  Train_Data = Data['Adj Close'][Data['Date'] < Date].to_numpy()
  Data_Train = []
  Data_Train_X = []
  Data_Train_Y = []
  for i in range(0, len(Train_Data), 5):
    try:
      Data_Train.append(Train_Data[i : i + 5])
    except:
      pass

  if len(Data_Train[-1]) < 5:
    Data_Train.pop(-1)

  Data_Train_X = Data_Train[0 : -1]
  Data_Train_X = np.array(Data_Train_X)
  Data_Train_X = Data_Train_X.reshape((-1, 5, 1))
  Data_Train_Y = Data_Train[1 : len(Data_Train)]
  Data_Train_Y = np.array(Data_Train_Y)
  Data_Train_Y = Data_Train_Y.reshape((-1, 5, 1))


  Test_Data = Data['Adj Close'][Data['Date'] >= Date].to_numpy()
  Data_Test = []
  Data_Test_X = []
  Data_Test_Y = []
  for i in range(0, len(Test_Data), 5):
    try:
      Data_Test.append(Test_Data[i : i + 5])
    except:
      pass

  if len(Data_Test[-1]) < 5:
    Data_Test.pop(-1)

  Data_Test_X = Data_Test[0 : -1]
  Data_Test_X = np.array(Data_Test_X)
  Data_Test_X = Data_Test_X.reshape((-1, 5, 1))
  Data_Test_Y = Data_Test[1 : len(Data_Test)]
  Data_Test_Y = np.array(Data_Test_Y)
  Data_Test_Y = Data_Test_Y.reshape((-1, 5, 1))

  return Data_Train_X, Data_Train_Y, Data_Test_X, Data_Test_Y

def Model():
  model = tf.keras.models.Sequential([
                                      tf.keras.layers.LSTM(200, input_shape = (5, 1), activation = tf.nn.leaky_relu, return_sequences = True),
                                      tf.keras.layers.LSTM(200, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(200, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(100, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(50, activation = tf.nn.leaky_relu),
                                      tf.keras.layers.Dense(5, activation = tf.nn.leaky_relu)
                                      ])
  return model

model = Model()

tf.keras.utils.plot_model(model, show_shapes=True)

model.summary()

def scheduler(epoch):

  if epoch <= 150:
    lrate = (10 ** -5) * (epoch / 150)
  elif epoch <= 400:
    initial_lrate = (10 ** -5)
    k = 0.01
    lrate = initial_lrate * math.exp(-k * (epoch - 150))
  else:
    lrate = (10 ** -6)

  return lrate

epochs = [i for i in range(1, 1001, 1)]
lrate = [scheduler(i) for i in range(1, 1001, 1)]
plt.plot(epochs, lrate)

callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

SP500_Date = '2020-10-01'
SP500_Train_X, SP500_Train_Y, SP500_Test_X, SP500_Test_Y = Dataset(SP500, SP500_Date)

SP500_Model = Model()

SP500_Model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'mse', metrics = tf.keras.metrics.RootMeanSquaredError())

SP500_hist = SP500_Model.fit(SP500_Train_X, SP500_Train_Y, epochs = 260, validation_data = (SP500_Test_X, SP500_Test_Y), callbacks=[callback])

history_dict = SP500_hist.history

loss = history_dict["loss"]
root_mean_squared_error = history_dict["root_mean_squared_error"]
val_loss = history_dict["val_loss"]
val_root_mean_squared_error = history_dict["val_root_mean_squared_error"]

epochs = range(1, len(loss) + 1)

fig, (ax1, ax2) = plt.subplots(1, 2)

fig.set_figheight(5)
fig.set_figwidth(15)

ax1.plot(epochs, loss, label = 'Training Loss')
ax1.plot(epochs, val_loss, label = 'Validation Loss')
ax1.set(xlabel = "Epochs", ylabel = "Loss")
ax1.legend()

ax2.plot(epochs, root_mean_squared_error, label = "Training Root Mean Squared Error")
ax2.plot(epochs, val_root_mean_squared_error, label = "Validation Root Mean Squared Error")
ax2.set(xlabel = "Epochs", ylabel = "Loss")
ax2.legend()

plt.show()

SP500_prediction = SP500_Model.predict(SP500_Test_X)

plt.figure(figsize=(20, 5))
plt.plot(SP500['Date'][SP500['Date'] >= '2020-10-09'], SP500['Adj Close'][SP500['Date'] >= '2020-10-09'], label='Testing')
plt.plot(SP500['Date'][SP500['Date'] >= '2020-10-12'][:-1], SP500_prediction.reshape(-1)[:-2], label='Predictions')
plt.xlabel('Time')
plt.ylabel('Closing Price')
plt.legend(loc='best')
plt.show()

# Assuming SP500_Test_Y and SP500_prediction are NumPy arrays
rmse = math.sqrt(mean_squared_error(SP500_Test_Y.reshape(-1, 5), SP500_prediction))
mape = np.mean(np.abs(SP500_prediction - SP500_Test_Y.reshape(-1, 5)) / np.abs(SP500_Test_Y.reshape(-1, 5)))
r_squared = r2_score(SP500_Test_Y.reshape(-1, 5), SP500_prediction)

# Number of observations
n = len(SP500_Test_Y.reshape(-1, 5))

# Number of features (assuming 5 features based on the reshape)
k = 5

# Calculate adjusted R-squared
adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)

print(f'RMSE: {rmse}')
print(f'MAPE: {mape}')
print(f'Adjusted R-squared: {adjusted_r_squared}')

